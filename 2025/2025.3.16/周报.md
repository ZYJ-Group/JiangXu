## 周报
 -  **本周任务** 
 1.验证一些持续学习方法
 2.在关键点检测-持续学习任务上加入了EWC、网络结构冻结、数据回放（原始数据内存缓冲区）、知识蒸馏方法
 

## 完成情况

 1.用world_view数据集作为第一个任务训练了一个关键点检测的模型作为教师模型，在模型中加入了上述方法，在TG数据集上训练，初步测试如下：
 **1.未使用持续学习在多目标下的情况**
 评价指标计算是label和预测点间的欧式距离
 
 ![](/2025/2025.3.16/img/1.bmp) ![](/2025/2025.3.16/img/2.bmp) ![](/2025/2025.3.16/img/3.bmp)![](/2025/2025.3.16/img/4.bmp)
 ![](/2025/2025.3.16/img/5.bmp) ![](/2025/2025.3.16/img/6.bmp) ![](/2025/2025.3.16/img/7.bmp)![](/2025/2025.3.16/img/8.bmp)
  **2.使用EWC、知识蒸馏在多目标下的情况**
  ![](/2025/2025.3.16/img/9.bmp)![](/2025/2025.3.16/img/10.bmp)![](/2025/2025.3.16/img/11.bmp)![](/2025/2025.3.16/img/12.bmp)
  ![](/2025/2025.3.16/img/13.bmp)![](/2025/2025.3.16/img/14.bmp)![](/2025/2025.3.16/img/15.bmp)![](/2025/2025.3.16/img/16.bmp)
  ![输入图片说明](/2025/2025.3.16/img/17.bmp)
  
  
 - 无论怎么去调整EWC和知识蒸馏等参数总的误差都在90以上，但可以调整参数使得新旧任务的表现相反，所以单依靠计算参数的重要度和知识蒸馏是无法很好的提高精度，毕竟参数还是在变化的
  **3.加入数据回放和某些网络层冻结的方法在多目标下的情况**
  ![输入图片说明](/2025/2025.3.16/img/18.bmp)![输入图片说明](/2025/2025.3.16/img/19.bmp)![输入图片说明](/2025/2025.3.16/img/20.bmp)![输入图片说明](/2025/2025.3.16/img/21.bmp)![输入图片说明](/2025/2025.3.16/img/22.bmp)
  ![输入图片说明](/2025/2025.3.16/img/23.bmp)![输入图片说明](/2025/2025.3.16/img/24.bmp)![输入图片说明](/2025/2025.3.16/img/25.bmp)
  
 - 通过和上述实验结果比较说明数据回放效果很好，可以在这个点上添加带数据压缩的自适应记忆回放策略

## 想法

 1. EWC正则化方法只是计算Fisher矩阵在保护重要的参数不被更改的太多，但是当更多的任务时参数总是会改变的，实验结果证明无法平衡新旧任务上的表现；而知识蒸馏会依赖旧模型输出，在更多任务叠加进来的时候就会一步步丧失精度,受持续学习相关论文按照任务扩展FC层的方式启发，可以从拓展网络参数的方面入手（可扩展神经网络），在结合上述方法做实验验证
 ![输入图片说明](/2025/2025.3.16/img/26.bmp)

 2.识别目标类型按照物理结构特征对网络模型输出的矫正（输出的热力图选取概率值大的置信度高的点按照结构特征对置信度低的点进行修正）

  


 

